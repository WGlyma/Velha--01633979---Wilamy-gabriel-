Nome do arquivo: 
  Data de cria√ß√£o: 23/05/2025
  Autor: Wilamy Gabriel
  Matr√≠cula : 01633979


‚ùå Jogo da Velha com Q-Learning


Projeto de aprendizado por refor√ßo que treina um agente para jogar jogo da velha contra um oponente aleat√≥rio. O agente aprende estrat√©gias para vencer, empatar e evitar derrotas, atualizando sua Q-Table dinamicamente.

---

üìå Objetivo



Treinar um agente capaz de jogar de forma inteligente, utilizando Q-Learning para maximizar vit√≥rias e minimizar derrotas, aprendendo a partir das recompensas e penalidades.

---

üõ†Ô∏è Tecnologias e Bibliotecas Utilizadas


Python 3.x

Pygame (interface gr√°fica)

Numpy (opcional para manipula√ß√£o de dados)
---

##üìò Algoritmo Utilizado


Q-Learning com estrat√©gia Œµ-greedy para balancear explora√ß√£o e explora√ß√£o.

**F√≥rmula de atualiza√ß√£o**:
Q[s][a] = Q[s][a] + Œ± * (recompensa + Œ≥ * max(Q[s']) - Q[s][a])

- Œ± (alpha): taxa de aprendizado

- Œ≥ (gamma): fator de desconto

- Œµ (epsilon): taxa de explora√ß√£o (decai ao longo do tempo)



üß™ Como Executar o Projeto


1. Clonar o reposit√≥rio




git clone https://github.com/WGlyma/Velha--01633979---Wilamy-gabriel-.git
cd Velha--01633979---Wilamy-gabriel-




2- Instalar as depend√™ncias
pip install pygame numpy




3- Executar o script principal
python jogo_tictactoe.py 

---



üìä Resultados Obtidos


Aprendizado progressivo do agente, aumentando o n√∫mero de vit√≥rias

Diminui√ß√£o das derrotas e empates ao longo do treinamento

Visualiza√ß√£o gr√°fica da partida em tempo real com placar atualizado

---

üß† Dificuldades Encontradas

Ajuste da taxa de explora√ß√£o para evitar que o agente fique preso em estrat√©gias ruins

Definir recompensas e penalidades que guiem o aprendizado de forma eficiente

Balancear a velocidade do treinamento com a qualidade do aprendizado

---

‚úÖ Coment√°rios Finais

O projeto demonstra na pr√°tica os conceitos de aprendizado por refor√ßo com Q-Learning, incluindo a explora√ß√£o vs. explora√ß√£o, atualiza√ß√£o din√¢mica da Q-Table e aprendizado com recompensas e penalidades.

---


üìà Apresenta√ß√£o Visual
Durante o treinamento do agente, foram gerados gr√°ficos e prints que ilustram a evolu√ß√£o do desempenho, mostrando o n√∫mero de vit√≥rias, derrotas e empates ao longo dos epis√≥dios. Essas visualiza√ß√µes ajudam a entender como o agente melhora seu aprendizado com o tempo e a efic√°cia do algoritmo aplicado.


![image](https://github.com/user-attachments/assets/ec04a0ca-52c4-4ae5-84f4-f38a9179c29d)

Desempenho do Agente Durante o Treinamento:

O gr√°fico acima ilustra a performance do agente de aprendizado por refor√ßo durante o processo de treinamento no ambiente do jogo da velha. O eixo X representa a quantidade de epis√≥dios (partidas jogadas), enquanto o eixo Y mostra a quantidade de resultados alcan√ßados (vit√≥rias, derrotas e empates) a cada intervalo de epis√≥dios.

Vit√≥rias (linha verde): O n√∫mero de vit√≥rias do agente aumenta de forma significativa √† medida que o treinamento avan√ßa, estabilizando-se acima de 80 vit√≥rias por lote de epis√≥dios. Isso indica que o agente est√° aprendendo estrat√©gias eficazes para vencer o jogo.


Derrotas (linha vermelha): A quantidade de derrotas cai rapidamente nas primeiras itera√ß√µes e se mant√©m baixa ao longo do tempo. Isso mostra que o agente est√° deixando de cometer erros comuns e aprendendo a evitar situa√ß√µes de perda.


Empates (linha azul): Os empates ocorrem com menor frequ√™ncia e se mant√™m relativamente est√°veis, o que √© comum em jogos como o da velha, onde muitas jogadas perfeitas terminam empatadas.


Esse desempenho demonstra que o agente est√° evoluindo com sucesso durante o treinamento, tornando-se cada vez mais eficiente em alcan√ßar vit√≥rias e minimizar derrotas.

---




![image](https://github.com/user-attachments/assets/36d07e36-2192-4986-b4c7-90d486c47626)








Execu√ß√£o do Jogo da Velha com Agente Q-Learning:


A imagem mostra a execu√ß√£o em tempo real do jogo da velha, onde um agente treinado com Q-Learning enfrenta um jogador humano. O ambiente foi implementado com a biblioteca Pygame, permitindo uma interface gr√°fica interativa.


Janela do Jogo


Na parte central da tela, vemos a interface gr√°fica do jogo:


O jogador humano est√° utilizando o s√≠mbolo "X", e o agente, o "O".
Abaixo do tabuleiro, aparece o placar atual da partida:
Jogador: 2 | Agente: 0, indicando que o jogador venceu duas partidas, enquanto o agente ainda n√£o venceu nesta execu√ß√£o.
Terminal (lado esquerdo)


O terminal exibe o desempenho do agente durante o treinamento:


A cada 100 epis√≥dios, s√£o exibidas as estat√≠sticas de vit√≥rias, derrotas e empates.
Os resultados mostram uma clara evolu√ß√£o no desempenho do agente, com aumento no n√∫mero de vit√≥rias e queda no n√∫mero de derrotas e empates.
Por exemplo:
No epis√≥dio 100: Vit√≥rias=56, Derrotas=33, Empates=11
No epis√≥dio 4200: Vit√≥rias=81, Derrotas=15, Empates=4


Esses dados refor√ßam que o agente est√° aprendendo com suas experi√™ncias e se tornando cada vez mais eficiente no jogo.
