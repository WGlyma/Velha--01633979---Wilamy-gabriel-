Nome do arquivo: 
 * Data de cria√ß√£o: 23/05/2025
 * Autor: Wilamy Gabriel
 * Matr√≠cula: 01633979


‚ùå Jogo da Velha com Q-Learning


Projeto de aprendizado por refor√ßo que treina um agente para jogar jogo da velha contra um oponente aleat√≥rio. O agente aprende estrat√©gias para vencer, empatar e evitar derrotas, atualizando sua Q-Table dinamicamente.


üìå Objetivo



Treinar um agente capaz de jogar de forma inteligente, utilizando Q-Learning para maximizar vit√≥rias e minimizar derrotas, aprendendo a partir das recompensas e penalidades.


üõ†Ô∏è Tecnologias e Bibliotecas Utilizadas


Python 3.x

Pygame (interface gr√°fica)

Numpy (opcional para manipula√ß√£o de dados)


üìò Algoritmo Utilizado


Q-Learning com estrat√©gia Œµ-greedy para balancear explora√ß√£o e explora√ß√£o.

F√≥rmula de atualiza√ß√£o:
Q[s][a] = Q[s][a] + Œ± * (recompensa + Œ≥ * max(Q[s']) - Q[s][a])

Œ± (alpha): taxa de aprendizado

Œ≥ (gamma): fator de desconto

Œµ (epsilon): taxa de explora√ß√£o (decai ao longo do tempo)


üß™ Como Executar o Projeto


Clonar o reposit√≥rio



Copiar
Editar
git clone <URL-do-seu-repo>
cd <pasta-do-repo>
Instalar depend√™ncias



nginx
Copiar
Editar
pip install pygame numpy
Executar o script principal



nginx
Copiar
Editar
python jogo_tictactoe.py



üìä Resultados Obtidos


Aprendizado progressivo do agente, aumentando o n√∫mero de vit√≥rias

Diminui√ß√£o das derrotas e empates ao longo do treinamento

Visualiza√ß√£o gr√°fica da partida em tempo real com placar atualizado


üß† Dificuldades Encontradas

Ajuste da taxa de explora√ß√£o para evitar que o agente fique preso em estrat√©gias ruins

Definir recompensas e penalidades que guiem o aprendizado de forma eficiente

Balancear a velocidade do treinamento com a qualidade do aprendizado


‚úÖ Coment√°rios Finais

O projeto demonstra na pr√°tica os conceitos de aprendizado por refor√ßo com Q-Learning, incluindo a explora√ß√£o vs. explora√ß√£o, atualiza√ß√£o din√¢mica da Q-Table e aprendizado com recompensas e penalidades.



üìà Apresenta√ß√£o Visual
Durante o treinamento do agente, foram gerados gr√°ficos e prints que ilustram a evolu√ß√£o do desempenho, mostrando o n√∫mero de vit√≥rias, derrotas e empates ao longo dos epis√≥dios. Essas visualiza√ß√µes ajudam a entender como o agente melhora seu aprendizado com o tempo e a efic√°cia do algoritmo aplicado.


![image](https://github.com/user-attachments/assets/ec04a0ca-52c4-4ae5-84f4-f38a9179c29d)

Desempenho do Agente Durante o Treinamento:

O gr√°fico acima ilustra a performance do agente de aprendizado por refor√ßo durante o processo de treinamento no ambiente do jogo da velha. O eixo X representa a quantidade de epis√≥dios (partidas jogadas), enquanto o eixo Y mostra a quantidade de resultados alcan√ßados (vit√≥rias, derrotas e empates) a cada intervalo de epis√≥dios.

Vit√≥rias (linha verde): O n√∫mero de vit√≥rias do agente aumenta de forma significativa √† medida que o treinamento avan√ßa, estabilizando-se acima de 80 vit√≥rias por lote de epis√≥dios. Isso indica que o agente est√° aprendendo estrat√©gias eficazes para vencer o jogo.
Derrotas (linha vermelha): A quantidade de derrotas cai rapidamente nas primeiras itera√ß√µes e se mant√©m baixa ao longo do tempo. Isso mostra que o agente est√° deixando de cometer erros comuns e aprendendo a evitar situa√ß√µes de perda.
Empates (linha azul): Os empates ocorrem com menor frequ√™ncia e se mant√™m relativamente est√°veis, o que √© comum em jogos como o da velha, onde muitas jogadas perfeitas terminam empatadas.
Esse desempenho demonstra que o agente est√° evoluindo com sucesso durante o treinamento, tornando-se cada vez mais eficiente em alcan√ßar vit√≥rias e minimizar derrotas.
